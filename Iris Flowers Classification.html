import warnings 
warnings.filterwarnings('ignore')
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

import seaborn as sns
sns.set(style ="white",color_codes=True)
iris =pd.read_csv("C:/Users/Sumant kumar/Documents/iris.csv")
iris.head()
sepal_length	sepal_width	petal_length	petal_width	species
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.7	3.2	1.3	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
# to display stats about data
iris.describe()
sepal_length	sepal_width	petal_length	petal_width
count	150.000000	150.000000	150.000000	150.000000
mean	5.843333	3.054000	3.758667	1.198667
std	0.828066	0.433594	1.764420	0.763161
min	4.300000	2.000000	1.000000	0.100000
25%	5.100000	2.800000	1.600000	0.300000
50%	5.800000	3.000000	4.350000	1.300000
75%	6.400000	3.300000	5.100000	1.800000
max	7.900000	4.400000	6.900000	2.500000
iris.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   sepal_length  150 non-null    float64
 1   sepal_width   150 non-null    float64
 2   petal_length  150 non-null    float64
 3   petal_width   150 non-null    float64
 4   species       150 non-null    object 
dtypes: float64(4), object(1)
memory usage: 6.0+ KB
iris["species"].value_counts()
setosa        50
versicolor    50
virginica     50
Name: species, dtype: int64
Perprocessing the dataset
# check for null values
iris.isnull().sum()
sepal_length    0
sepal_width     0
petal_length    0
petal_width     0
species         0
dtype: int64
iris['sepal_length'].hist()
<Axes: >

iris['sepal_width'].hist()
<Axes: >

iris['petal_length'].hist()
<Axes: >

iris['petal_width'].hist()
<Axes: >

Scatter Plot
sns.FacetGrid(iris, hue="species",height=6).map(plt.scatter,"petal_length","sepal_width").add_legend()
<seaborn.axisgrid.FacetGrid at 0x212b5024c10>

Coorelation Matrix
iris.corr()
sepal_length	sepal_width	petal_length	petal_width
sepal_length	1.000000	-0.109369	0.871754	0.817954
sepal_width	-0.109369	1.000000	-0.420516	-0.356544
petal_length	0.871754	-0.420516	1.000000	0.962757
petal_width	0.817954	-0.356544	0.962757	1.000000
corr=iris.corr()
fig, ax =plt.subplots(figsize=(5,4))
sns.heatmap(corr,annot=True ,ax=ax)
<Axes: >

Logistic Regression
Coverting categorical varibales into number
flower_mapping={'setosa':0,'versicolor':1,'virginica':2}
iris['species']=iris['species'].map(flower_mapping)
iris.head()
sepal_length	sepal_width	petal_length	petal_width	species
0	5.1	3.5	1.4	0.2	0
1	4.9	3.0	1.4	0.2	0
2	4.7	3.2	1.3	0.2	0
3	4.6	3.1	1.5	0.2	0
4	5.0	3.6	1.4	0.2	0
Preparing inputs and outputs
x=iris[['sepal_length','sepal_width','petal_length','petal_width']].values
y=iris[['species']].values
Model Training
from sklearn.model_selection import train_test_split
x=iris.drop(columns=['species'])
y=iris['species']
x_train,x_test,y_train,y_test=train_test_split(x,y)
Logistic Regression
from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
model.fit(x_train,y_train)

LogisticRegression
LogisticRegression()
Accuracy
model.score(x,y)
0.98
from sklearn.neighbors import KNeighborsClassifier
model=KNeighborsClassifier()
model.fit(x_train,y_train)

KNeighborsClassifier
KNeighborsClassifier()
print("Accuracy",model.score(x_test,y_test)*100)
Accuracy 97.36842105263158
Make Predictions
expected=y
predicted = model.predict(x)
predicted
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)
summarize the fit of the model
from sklearn import metrics
print(metrics.classification_report(expected,predicted))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        50
           1       0.98      0.94      0.96        50
           2       0.94      0.98      0.96        50

    accuracy                           0.97       150
   macro avg       0.97      0.97      0.97       150
weighted avg       0.97      0.97      0.97       150

print(metrics.confusion_matrix(expected,predicted))
[[50  0  0]
 [ 0 47  3]
 [ 0  1 49]]
 
